{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import DataGenerator\n",
    "\n",
    "# Example usage\n",
    "generator = DataGenerator()\n",
    "\n",
    "# Generate training and test data for a specific type\n",
    "train_data, test_data = generator.generate_random_dataset(choice=0)  # 0 for circle dataset\n",
    "\n",
    "# Generate a mini-batch from training data\n",
    "batch = generator.generate_batch(train_data)\n",
    "batch_input, batch_output = batch[:, :2], batch[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 0.842182993888855\n",
      "Step 2, Loss: 0.8362444043159485\n",
      "Step 3, Loss: 0.8305174112319946\n",
      "Step 4, Loss: 0.8249969482421875\n",
      "Step 5, Loss: 0.8196778297424316\n",
      "Step 6, Loss: 0.8145545721054077\n",
      "Step 7, Loss: 0.8096219301223755\n",
      "Step 8, Loss: 0.80487459897995\n",
      "Step 9, Loss: 0.8003073930740356\n",
      "Step 10, Loss: 0.7959151268005371\n",
      "Step 11, Loss: 0.7916927337646484\n",
      "Step 12, Loss: 0.7876350283622742\n",
      "Step 13, Loss: 0.7837368249893188\n",
      "Step 14, Loss: 0.7799933552742004\n",
      "Step 15, Loss: 0.776399552822113\n",
      "Step 16, Loss: 0.7729504108428955\n",
      "Step 17, Loss: 0.7696412205696106\n",
      "Step 18, Loss: 0.7664671540260315\n",
      "Step 19, Loss: 0.7634236216545105\n",
      "Step 20, Loss: 0.7605057954788208\n",
      "Step 21, Loss: 0.7577095031738281\n",
      "Step 22, Loss: 0.7550300359725952\n",
      "Step 23, Loss: 0.7524630427360535\n",
      "Step 24, Loss: 0.7500045895576477\n",
      "Step 25, Loss: 0.7476502656936646\n",
      "Step 26, Loss: 0.7453961968421936\n",
      "Step 27, Loss: 0.7432383298873901\n",
      "Step 28, Loss: 0.7411730289459229\n",
      "Step 29, Loss: 0.7391964197158813\n",
      "Step 30, Loss: 0.7373050451278687\n",
      "Step 31, Loss: 0.7354954481124878\n",
      "Step 32, Loss: 0.7337640523910522\n",
      "Step 33, Loss: 0.7321078181266785\n",
      "Step 34, Loss: 0.7305235862731934\n",
      "Step 35, Loss: 0.7290081977844238\n",
      "Step 36, Loss: 0.7275587916374207\n",
      "Step 37, Loss: 0.7261725664138794\n",
      "Step 38, Loss: 0.7248467803001404\n",
      "Step 39, Loss: 0.7235788106918335\n",
      "Step 40, Loss: 0.722366213798523\n",
      "Step 41, Loss: 0.7212064862251282\n",
      "Step 42, Loss: 0.7200973033905029\n",
      "Step 43, Loss: 0.7190310955047607\n",
      "Step 44, Loss: 0.7180114388465881\n",
      "Step 45, Loss: 0.7170361876487732\n",
      "Step 46, Loss: 0.7161036729812622\n",
      "Step 47, Loss: 0.7152117490768433\n",
      "Step 48, Loss: 0.7143587470054626\n",
      "Step 49, Loss: 0.7135427594184875\n",
      "Step 50, Loss: 0.7127622365951538\n",
      "Step 51, Loss: 0.7120155692100525\n",
      "Step 52, Loss: 0.7113012075424194\n",
      "Step 53, Loss: 0.7106176614761353\n",
      "Step 54, Loss: 0.7099636793136597\n",
      "Step 55, Loss: 0.7093378901481628\n",
      "Step 56, Loss: 0.7087389230728149\n",
      "Step 57, Loss: 0.7081656455993652\n",
      "Step 58, Loss: 0.707616925239563\n",
      "Step 59, Loss: 0.7070916295051575\n",
      "Step 60, Loss: 0.706588625907898\n",
      "Step 61, Loss: 0.7061070203781128\n",
      "Step 62, Loss: 0.7056458592414856\n",
      "Step 63, Loss: 0.7052041888237\n",
      "Step 64, Loss: 0.7047811150550842\n",
      "Step 65, Loss: 0.7043758034706116\n",
      "Step 66, Loss: 0.7039875388145447\n",
      "Step 67, Loss: 0.7036154270172119\n",
      "Step 68, Loss: 0.7032588124275208\n",
      "Step 69, Loss: 0.7029170989990234\n",
      "Step 70, Loss: 0.7025896310806274\n",
      "Step 71, Loss: 0.7022755146026611\n",
      "Step 72, Loss: 0.7019743919372559\n",
      "Step 73, Loss: 0.7016855478286743\n",
      "Step 74, Loss: 0.7014086246490479\n",
      "Step 75, Loss: 0.7011429667472839\n",
      "Step 76, Loss: 0.7008882164955139\n",
      "Step 77, Loss: 0.7006436586380005\n",
      "Step 78, Loss: 0.7004090547561646\n",
      "Step 79, Loss: 0.7001837491989136\n",
      "Step 80, Loss: 0.699967622756958\n",
      "Step 81, Loss: 0.6997600793838501\n",
      "Step 82, Loss: 0.699560821056366\n",
      "Step 83, Loss: 0.6993694305419922\n",
      "Step 84, Loss: 0.6991855502128601\n",
      "Step 85, Loss: 0.6990090012550354\n",
      "Step 86, Loss: 0.6988394260406494\n",
      "Step 87, Loss: 0.6986762881278992\n",
      "Step 88, Loss: 0.6985195875167847\n",
      "Step 89, Loss: 0.6983690857887268\n",
      "Step 90, Loss: 0.6982243061065674\n",
      "Step 91, Loss: 0.6980850100517273\n",
      "Step 92, Loss: 0.6979511380195618\n",
      "Step 93, Loss: 0.6978223919868469\n",
      "Step 94, Loss: 0.6976984739303589\n",
      "Step 95, Loss: 0.6975792646408081\n",
      "Step 96, Loss: 0.6974645853042603\n",
      "Step 97, Loss: 0.6973550319671631\n",
      "Step 98, Loss: 0.6972496509552002\n",
      "Step 99, Loss: 0.6971480250358582\n",
      "Step 100, Loss: 0.6970503330230713\n",
      "Step 101, Loss: 0.6969561576843262\n",
      "Step 102, Loss: 0.696865439414978\n",
      "Step 103, Loss: 0.6967781186103821\n",
      "Step 104, Loss: 0.6966938376426697\n",
      "Step 105, Loss: 0.6966127157211304\n",
      "Step 106, Loss: 0.6965345144271851\n",
      "Step 107, Loss: 0.6964591145515442\n",
      "Step 108, Loss: 0.6963863372802734\n",
      "Step 109, Loss: 0.6963162422180176\n",
      "Step 110, Loss: 0.6962485313415527\n",
      "Step 111, Loss: 0.6961832046508789\n",
      "Step 112, Loss: 0.6961201429367065\n",
      "Step 113, Loss: 0.6960593461990356\n",
      "Step 114, Loss: 0.6960004568099976\n",
      "Step 115, Loss: 0.6959437727928162\n",
      "Step 116, Loss: 0.6958889961242676\n",
      "Step 117, Loss: 0.6958360075950623\n",
      "Step 118, Loss: 0.6957848072052002\n",
      "Step 119, Loss: 0.6957353949546814\n",
      "Step 120, Loss: 0.6956876516342163\n",
      "Step 121, Loss: 0.6956413388252258\n",
      "Step 122, Loss: 0.6955966353416443\n",
      "Step 123, Loss: 0.6955533623695374\n",
      "Step 124, Loss: 0.6955114603042603\n",
      "Step 125, Loss: 0.695470929145813\n",
      "Step 126, Loss: 0.6954317092895508\n",
      "Step 127, Loss: 0.6953936815261841\n",
      "Step 128, Loss: 0.6953569650650024\n",
      "Step 129, Loss: 0.6953213214874268\n",
      "Step 130, Loss: 0.6952866911888123\n",
      "Step 131, Loss: 0.6952532529830933\n",
      "Step 132, Loss: 0.6952207684516907\n",
      "Step 133, Loss: 0.6951892971992493\n",
      "Step 134, Loss: 0.6951587200164795\n",
      "Step 135, Loss: 0.6951291561126709\n",
      "Step 136, Loss: 0.6951003670692444\n",
      "Step 137, Loss: 0.6950724720954895\n",
      "Step 138, Loss: 0.6950453519821167\n",
      "Step 139, Loss: 0.6950190663337708\n",
      "Step 140, Loss: 0.6949934363365173\n",
      "Step 141, Loss: 0.694968581199646\n",
      "Step 142, Loss: 0.6949443817138672\n",
      "Step 143, Loss: 0.6949209570884705\n",
      "Step 144, Loss: 0.6948981285095215\n",
      "Step 145, Loss: 0.6948758363723755\n",
      "Step 146, Loss: 0.6948542594909668\n",
      "Step 147, Loss: 0.6948331594467163\n",
      "Step 148, Loss: 0.6948127150535583\n",
      "Step 149, Loss: 0.6947928071022034\n",
      "Step 150, Loss: 0.6947733163833618\n",
      "Step 151, Loss: 0.6947543621063232\n",
      "Step 152, Loss: 0.6947358846664429\n",
      "Step 153, Loss: 0.6947179436683655\n",
      "Step 154, Loss: 0.6947003602981567\n",
      "Step 155, Loss: 0.6946831941604614\n",
      "Step 156, Loss: 0.6946665644645691\n",
      "Step 157, Loss: 0.6946501135826111\n",
      "Step 158, Loss: 0.694634199142456\n",
      "Step 159, Loss: 0.6946186423301697\n",
      "Step 160, Loss: 0.6946035027503967\n",
      "Step 161, Loss: 0.6945886611938477\n",
      "Step 162, Loss: 0.6945741772651672\n",
      "Step 163, Loss: 0.6945599317550659\n",
      "Step 164, Loss: 0.6945460438728333\n",
      "Step 165, Loss: 0.6945325136184692\n",
      "Step 166, Loss: 0.6945191621780396\n",
      "Step 167, Loss: 0.6945061683654785\n",
      "Step 168, Loss: 0.6944935321807861\n",
      "Step 169, Loss: 0.6944810152053833\n",
      "Step 170, Loss: 0.6944687962532043\n",
      "Step 171, Loss: 0.6944568753242493\n",
      "Step 172, Loss: 0.6944451332092285\n",
      "Step 173, Loss: 0.6944335699081421\n",
      "Step 174, Loss: 0.6944224238395691\n",
      "Step 175, Loss: 0.6944112777709961\n",
      "Step 176, Loss: 0.6944003701210022\n",
      "Step 177, Loss: 0.694389820098877\n",
      "Step 178, Loss: 0.6943793296813965\n",
      "Step 179, Loss: 0.6943690776824951\n",
      "Step 180, Loss: 0.6943589448928833\n",
      "Step 181, Loss: 0.6943491697311401\n",
      "Step 182, Loss: 0.694339394569397\n",
      "Step 183, Loss: 0.6943297386169434\n",
      "Step 184, Loss: 0.6943203806877136\n",
      "Step 185, Loss: 0.6943112015724182\n",
      "Step 186, Loss: 0.6943020820617676\n",
      "Step 187, Loss: 0.6942930817604065\n",
      "Step 188, Loss: 0.6942844390869141\n",
      "Step 189, Loss: 0.6942756772041321\n",
      "Step 190, Loss: 0.6942671537399292\n",
      "Step 191, Loss: 0.6942588090896606\n",
      "Step 192, Loss: 0.6942505240440369\n",
      "Step 193, Loss: 0.6942423582077026\n",
      "Step 194, Loss: 0.6942342519760132\n",
      "Step 195, Loss: 0.6942263841629028\n",
      "Step 196, Loss: 0.694218635559082\n",
      "Step 197, Loss: 0.6942110061645508\n",
      "Step 198, Loss: 0.6942034363746643\n",
      "Step 199, Loss: 0.6941959261894226\n",
      "Step 200, Loss: 0.6941885948181152\n",
      "Step 201, Loss: 0.6941813230514526\n",
      "Step 202, Loss: 0.6941742300987244\n",
      "Step 203, Loss: 0.6941670775413513\n",
      "Step 204, Loss: 0.6941601037979126\n",
      "Step 205, Loss: 0.6941532492637634\n",
      "Step 206, Loss: 0.6941463947296143\n",
      "Step 207, Loss: 0.6941397786140442\n",
      "Step 208, Loss: 0.6941331028938293\n",
      "Step 209, Loss: 0.6941264867782593\n",
      "Step 210, Loss: 0.6941200494766235\n",
      "Step 211, Loss: 0.6941136717796326\n",
      "Step 212, Loss: 0.6941072344779968\n",
      "Step 213, Loss: 0.6941011548042297\n",
      "Step 214, Loss: 0.6940948367118835\n",
      "Step 215, Loss: 0.6940888166427612\n",
      "Step 216, Loss: 0.6940827369689941\n",
      "Step 217, Loss: 0.6940767765045166\n",
      "Step 218, Loss: 0.6940708756446838\n",
      "Step 219, Loss: 0.6940650343894958\n",
      "Step 220, Loss: 0.6940591931343079\n",
      "Step 221, Loss: 0.6940535306930542\n",
      "Step 222, Loss: 0.6940478682518005\n",
      "Step 223, Loss: 0.6940422654151917\n",
      "Step 224, Loss: 0.6940366625785828\n",
      "Step 225, Loss: 0.6940311789512634\n",
      "Step 226, Loss: 0.6940257549285889\n",
      "Step 227, Loss: 0.6940203905105591\n",
      "Step 228, Loss: 0.6940150260925293\n",
      "Step 229, Loss: 0.6940097808837891\n",
      "Step 230, Loss: 0.6940045356750488\n",
      "Step 231, Loss: 0.6939993500709534\n",
      "Step 232, Loss: 0.6939941644668579\n",
      "Step 233, Loss: 0.6939891576766968\n",
      "Step 234, Loss: 0.6939840912818909\n",
      "Step 235, Loss: 0.6939791440963745\n",
      "Step 236, Loss: 0.6939741373062134\n",
      "Step 237, Loss: 0.6939692497253418\n",
      "Step 238, Loss: 0.6939644813537598\n",
      "Step 239, Loss: 0.6939595937728882\n",
      "Step 240, Loss: 0.6939547657966614\n",
      "Step 241, Loss: 0.6939500570297241\n",
      "Step 242, Loss: 0.6939454078674316\n",
      "Step 243, Loss: 0.6939407587051392\n",
      "Step 244, Loss: 0.6939361095428467\n",
      "Step 245, Loss: 0.6939315795898438\n",
      "Step 246, Loss: 0.693926990032196\n",
      "Step 247, Loss: 0.6939225196838379\n",
      "Step 248, Loss: 0.6939180493354797\n",
      "Step 249, Loss: 0.6939135789871216\n",
      "Step 250, Loss: 0.6939092874526978\n",
      "Step 251, Loss: 0.6939047574996948\n",
      "Step 252, Loss: 0.6939005851745605\n",
      "Step 253, Loss: 0.6938962936401367\n",
      "Step 254, Loss: 0.6938921213150024\n",
      "Step 255, Loss: 0.6938878297805786\n",
      "Step 256, Loss: 0.6938835978507996\n",
      "Step 257, Loss: 0.6938794851303101\n",
      "Step 258, Loss: 0.6938753128051758\n",
      "Step 259, Loss: 0.6938713192939758\n",
      "Step 260, Loss: 0.6938672065734863\n",
      "Step 261, Loss: 0.6938632130622864\n",
      "Step 262, Loss: 0.6938591599464417\n",
      "Step 263, Loss: 0.6938552260398865\n",
      "Step 264, Loss: 0.6938512325286865\n",
      "Step 265, Loss: 0.6938474178314209\n",
      "Step 266, Loss: 0.6938435435295105\n",
      "Step 267, Loss: 0.6938396692276001\n",
      "Step 268, Loss: 0.6938358545303345\n",
      "Step 269, Loss: 0.6938320398330688\n",
      "Step 270, Loss: 0.6938282251358032\n",
      "Step 271, Loss: 0.6938245892524719\n",
      "Step 272, Loss: 0.6938208341598511\n",
      "Step 273, Loss: 0.693817138671875\n",
      "Step 274, Loss: 0.6938134431838989\n",
      "Step 275, Loss: 0.6938098669052124\n",
      "Step 276, Loss: 0.6938062906265259\n",
      "Step 277, Loss: 0.6938026547431946\n",
      "Step 278, Loss: 0.6937991976737976\n",
      "Step 279, Loss: 0.6937956213951111\n",
      "Step 280, Loss: 0.6937920451164246\n",
      "Step 281, Loss: 0.6937885284423828\n",
      "Step 282, Loss: 0.6937850713729858\n",
      "Step 283, Loss: 0.6937817335128784\n",
      "Step 284, Loss: 0.6937783360481262\n",
      "Step 285, Loss: 0.6937748193740845\n",
      "Step 286, Loss: 0.693771481513977\n",
      "Step 287, Loss: 0.6937681436538696\n",
      "Step 288, Loss: 0.6937649250030518\n",
      "Step 289, Loss: 0.6937616467475891\n",
      "Step 290, Loss: 0.6937583088874817\n",
      "Step 291, Loss: 0.6937550902366638\n",
      "Step 292, Loss: 0.6937517523765564\n",
      "Step 293, Loss: 0.6937485933303833\n",
      "Step 294, Loss: 0.6937453746795654\n",
      "Step 295, Loss: 0.6937422156333923\n",
      "Step 296, Loss: 0.6937390565872192\n",
      "Step 297, Loss: 0.6937358975410461\n",
      "Step 298, Loss: 0.6937328577041626\n",
      "Step 299, Loss: 0.6937296986579895\n",
      "Step 300, Loss: 0.693726658821106\n",
      "Step 301, Loss: 0.6937235593795776\n",
      "Step 302, Loss: 0.6937205791473389\n",
      "Step 303, Loss: 0.6937175989151001\n",
      "Step 304, Loss: 0.6937146782875061\n",
      "Step 305, Loss: 0.6937116384506226\n",
      "Step 306, Loss: 0.6937087178230286\n",
      "Step 307, Loss: 0.6937057971954346\n",
      "Step 308, Loss: 0.6937028765678406\n",
      "Step 309, Loss: 0.6936999559402466\n",
      "Step 310, Loss: 0.6936970949172974\n",
      "Step 311, Loss: 0.6936942338943481\n",
      "Step 312, Loss: 0.6936914920806885\n",
      "Step 313, Loss: 0.693688690662384\n",
      "Step 314, Loss: 0.6936858296394348\n",
      "Step 315, Loss: 0.6936830282211304\n",
      "Step 316, Loss: 0.6936802864074707\n",
      "Step 317, Loss: 0.6936774253845215\n",
      "Step 318, Loss: 0.6936746835708618\n",
      "Step 319, Loss: 0.6936720609664917\n",
      "Step 320, Loss: 0.6936694383621216\n",
      "Step 321, Loss: 0.6936666965484619\n",
      "Step 322, Loss: 0.693664014339447\n",
      "Step 323, Loss: 0.6936613321304321\n",
      "Step 324, Loss: 0.693658709526062\n",
      "Step 325, Loss: 0.6936560869216919\n",
      "Step 326, Loss: 0.6936535239219666\n",
      "Step 327, Loss: 0.6936509609222412\n",
      "Step 328, Loss: 0.6936483383178711\n",
      "Step 329, Loss: 0.6936457753181458\n",
      "Step 330, Loss: 0.69364333152771\n",
      "Step 331, Loss: 0.6936408281326294\n",
      "Step 332, Loss: 0.6936383247375488\n",
      "Step 333, Loss: 0.6936357617378235\n",
      "Step 334, Loss: 0.6936334371566772\n",
      "Step 335, Loss: 0.6936308741569519\n",
      "Step 336, Loss: 0.6936284303665161\n",
      "Step 337, Loss: 0.6936260461807251\n",
      "Step 338, Loss: 0.6936236619949341\n",
      "Step 339, Loss: 0.6936212778091431\n",
      "Step 340, Loss: 0.693618893623352\n",
      "Step 341, Loss: 0.693616509437561\n",
      "Step 342, Loss: 0.69361412525177\n",
      "Step 343, Loss: 0.693611741065979\n",
      "Step 344, Loss: 0.6936095356941223\n",
      "Step 345, Loss: 0.6936072111129761\n",
      "Step 346, Loss: 0.6936049461364746\n",
      "Step 347, Loss: 0.6936025619506836\n",
      "Step 348, Loss: 0.6936002969741821\n",
      "Step 349, Loss: 0.6935980916023254\n",
      "Step 350, Loss: 0.6935957670211792\n",
      "Step 351, Loss: 0.6935936212539673\n",
      "Step 352, Loss: 0.6935914754867554\n",
      "Step 353, Loss: 0.6935893297195435\n",
      "Step 354, Loss: 0.6935870051383972\n",
      "Step 355, Loss: 0.6935849785804749\n",
      "Step 356, Loss: 0.6935827136039734\n",
      "Step 357, Loss: 0.6935805678367615\n",
      "Step 358, Loss: 0.6935785412788391\n",
      "Step 359, Loss: 0.6935763955116272\n",
      "Step 360, Loss: 0.6935741901397705\n",
      "Step 361, Loss: 0.6935721635818481\n",
      "Step 362, Loss: 0.693570077419281\n",
      "Step 363, Loss: 0.6935679912567139\n",
      "Step 364, Loss: 0.6935659646987915\n",
      "Step 365, Loss: 0.6935639381408691\n",
      "Step 366, Loss: 0.6935619115829468\n",
      "Step 367, Loss: 0.6935598254203796\n",
      "Step 368, Loss: 0.6935577988624573\n",
      "Step 369, Loss: 0.6935559511184692\n",
      "Step 370, Loss: 0.6935539245605469\n",
      "Step 371, Loss: 0.6935518980026245\n",
      "Step 372, Loss: 0.6935499906539917\n",
      "Step 373, Loss: 0.6935479640960693\n",
      "Step 374, Loss: 0.6935461163520813\n",
      "Step 375, Loss: 0.6935442090034485\n",
      "Step 376, Loss: 0.6935422420501709\n",
      "Step 377, Loss: 0.6935403347015381\n",
      "Step 378, Loss: 0.69353848695755\n",
      "Step 379, Loss: 0.6935365200042725\n",
      "Step 380, Loss: 0.6935347318649292\n",
      "Step 381, Loss: 0.6935329437255859\n",
      "Step 382, Loss: 0.6935310959815979\n",
      "Step 383, Loss: 0.6935291886329651\n",
      "Step 384, Loss: 0.6935274004936218\n",
      "Step 385, Loss: 0.6935255527496338\n",
      "Step 386, Loss: 0.6935237646102905\n",
      "Step 387, Loss: 0.693522036075592\n",
      "Step 388, Loss: 0.693520188331604\n",
      "Step 389, Loss: 0.6935185194015503\n",
      "Step 390, Loss: 0.6935166120529175\n",
      "Step 391, Loss: 0.693514883518219\n",
      "Step 392, Loss: 0.6935132145881653\n",
      "Step 393, Loss: 0.6935114860534668\n",
      "Step 394, Loss: 0.6935097575187683\n",
      "Step 395, Loss: 0.6935080289840698\n",
      "Step 396, Loss: 0.6935063600540161\n",
      "Step 397, Loss: 0.6935046911239624\n",
      "Step 398, Loss: 0.6935030221939087\n",
      "Step 399, Loss: 0.693501353263855\n",
      "Step 400, Loss: 0.6934996843338013\n",
      "Step 401, Loss: 0.6934980154037476\n",
      "Step 402, Loss: 0.6934964656829834\n",
      "Step 403, Loss: 0.6934947967529297\n",
      "Step 404, Loss: 0.6934932470321655\n",
      "Step 405, Loss: 0.6934915781021118\n",
      "Step 406, Loss: 0.6934899687767029\n",
      "Step 407, Loss: 0.6934882998466492\n",
      "Step 408, Loss: 0.6934868097305298\n",
      "Step 409, Loss: 0.6934852004051208\n",
      "Step 410, Loss: 0.6934835910797119\n",
      "Step 411, Loss: 0.6934820413589478\n",
      "Step 412, Loss: 0.6934804916381836\n",
      "Step 413, Loss: 0.693479061126709\n",
      "Step 414, Loss: 0.6934775114059448\n",
      "Step 415, Loss: 0.6934760212898254\n",
      "Step 416, Loss: 0.6934744715690613\n",
      "Step 417, Loss: 0.6934730410575867\n",
      "Step 418, Loss: 0.6934714913368225\n",
      "Step 419, Loss: 0.6934700608253479\n",
      "Step 420, Loss: 0.6934685707092285\n",
      "Step 421, Loss: 0.6934671401977539\n",
      "Step 422, Loss: 0.6934656500816345\n",
      "Step 423, Loss: 0.6934641599655151\n",
      "Step 424, Loss: 0.6934627294540405\n",
      "Step 425, Loss: 0.6934612989425659\n",
      "Step 426, Loss: 0.6934599280357361\n",
      "Step 427, Loss: 0.6934585571289062\n",
      "Step 428, Loss: 0.6934570670127869\n",
      "Step 429, Loss: 0.693455696105957\n",
      "Step 430, Loss: 0.6934543251991272\n",
      "Step 431, Loss: 0.6934529542922974\n",
      "Step 432, Loss: 0.6934515237808228\n",
      "Step 433, Loss: 0.6934500932693481\n",
      "Step 434, Loss: 0.6934486031532288\n",
      "Step 435, Loss: 0.6934471130371094\n",
      "Step 436, Loss: 0.6934456825256348\n",
      "Step 437, Loss: 0.6934442520141602\n",
      "Step 438, Loss: 0.6934428215026855\n",
      "Step 439, Loss: 0.6934413313865662\n",
      "Step 440, Loss: 0.6934398412704468\n",
      "Step 441, Loss: 0.6934384107589722\n",
      "Step 442, Loss: 0.6934370398521423\n",
      "Step 443, Loss: 0.693435549736023\n",
      "Step 444, Loss: 0.6934341788291931\n",
      "Step 445, Loss: 0.6934328079223633\n",
      "Step 446, Loss: 0.6934314370155334\n",
      "Step 447, Loss: 0.6934300661087036\n",
      "Step 448, Loss: 0.693428635597229\n",
      "Step 449, Loss: 0.6934272646903992\n",
      "Step 450, Loss: 0.6934259533882141\n",
      "Step 451, Loss: 0.6934245824813843\n",
      "Step 452, Loss: 0.6934232115745544\n",
      "Step 453, Loss: 0.6934218406677246\n",
      "Step 454, Loss: 0.6934205293655396\n",
      "Step 455, Loss: 0.6934191584587097\n",
      "Step 456, Loss: 0.6934179067611694\n",
      "Step 457, Loss: 0.6934165358543396\n",
      "Step 458, Loss: 0.6934152841567993\n",
      "Step 459, Loss: 0.6934139132499695\n",
      "Step 460, Loss: 0.6934126615524292\n",
      "Step 461, Loss: 0.6934113502502441\n",
      "Step 462, Loss: 0.6934100985527039\n",
      "Step 463, Loss: 0.6934088468551636\n",
      "Step 464, Loss: 0.6934076547622681\n",
      "Step 465, Loss: 0.6934062838554382\n",
      "Step 466, Loss: 0.693405032157898\n",
      "Step 467, Loss: 0.6934038400650024\n",
      "Step 468, Loss: 0.6934025287628174\n",
      "Step 469, Loss: 0.6934012174606323\n",
      "Step 470, Loss: 0.6934000253677368\n",
      "Step 471, Loss: 0.6933988928794861\n",
      "Step 472, Loss: 0.6933976411819458\n",
      "Step 473, Loss: 0.6933963894844055\n",
      "Step 474, Loss: 0.69339519739151\n",
      "Step 475, Loss: 0.6933940052986145\n",
      "Step 476, Loss: 0.6933927536010742\n",
      "Step 477, Loss: 0.6933915019035339\n",
      "Step 478, Loss: 0.6933903694152832\n",
      "Step 479, Loss: 0.6933891773223877\n",
      "Step 480, Loss: 0.693388044834137\n",
      "Step 481, Loss: 0.6933868527412415\n",
      "Step 482, Loss: 0.6933857202529907\n",
      "Step 483, Loss: 0.6933846473693848\n",
      "Step 484, Loss: 0.6933833956718445\n",
      "Step 485, Loss: 0.6933822631835938\n",
      "Step 486, Loss: 0.693381130695343\n",
      "Step 487, Loss: 0.6933799982070923\n",
      "Step 488, Loss: 0.6933788061141968\n",
      "Step 489, Loss: 0.6933777928352356\n",
      "Step 490, Loss: 0.6933766603469849\n",
      "Step 491, Loss: 0.6933754682540894\n",
      "Step 492, Loss: 0.6933743953704834\n",
      "Step 493, Loss: 0.6933733224868774\n",
      "Step 494, Loss: 0.6933721899986267\n",
      "Step 495, Loss: 0.6933711171150208\n",
      "Step 496, Loss: 0.69336998462677\n",
      "Step 497, Loss: 0.6933689117431641\n",
      "Step 498, Loss: 0.6933678388595581\n",
      "Step 499, Loss: 0.6933667063713074\n",
      "Step 500, Loss: 0.693365752696991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ 0.   ,  0.   ,  0.   , ..., -0.015, -0.096,  0.149],\n",
       "       [ 0.   ,  0.   ,  0.   , ..., -0.266,  0.004, -0.006],\n",
       "       [ 0.   ,  0.   ,  0.   , ..., -0.111, -0.171,  0.062],\n",
       "       ...,\n",
       "       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.152, -0.791],\n",
       "       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.178],\n",
       "       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use 'act' to backprop on a single wMat \n",
    "# -- what's input and output? input 2D point, output 2D logit for each class (2 class)\n",
    "nInput = 2\n",
    "nOutput = 2 \n",
    "\n",
    "from fineNeat import Ind \n",
    "\n",
    "# initialize individual with fixed shape \n",
    "ind = Ind.from_shapes([(nInput, 5), (5, nOutput)])\n",
    "ind.express()\n",
    "\n",
    "from jax import numpy as jnp \n",
    "wMat = jnp.copy(ind.wMat)\n",
    "aVec = jnp.array(ind.aVec)\n",
    "\n",
    "# convert 'act' into jax and conduct backprop \n",
    "from fineNeat.sneat_jax.ann import act\n",
    "from loss import cross_entropy_loss\n",
    "from jax import value_and_grad\n",
    "\n",
    "# Define a function that computes loss given weights\n",
    "def loss_fn(weights, aVec, nInput, nOutput, inputs, targets):\n",
    "    logits = act(weights, aVec, nInput, nOutput, inputs)\n",
    "    return cross_entropy_loss(logits, targets)\n",
    "\n",
    "# Jax backprop step function\n",
    "def step(wMat, aVec, nInput, nOutput, batch_input, batch_output, learning_rate=0.01):\n",
    "    # Compute gradient\n",
    "    loss_value, grads = value_and_grad(loss_fn)(wMat, aVec, nInput, nOutput, batch_input, batch_output)\n",
    "    \n",
    "    # Gradient descent step \n",
    "    wMat_updated = wMat - learning_rate * grads\n",
    "\n",
    "    return wMat_updated, loss_value\n",
    "\n",
    "def train(wMat, aVec, nInput, nOutput, batch_input, batch_output, learning_rate=0.01, num_steps=100):\n",
    "    for _ in range(num_steps):\n",
    "        wMat, loss_value = step(wMat, aVec, nInput, nOutput, batch_input, batch_output, learning_rate)\n",
    "        print(f\"Step {_ + 1}, Loss: {loss_value}\")\n",
    "    return wMat\n",
    "\n",
    "train(wMat, aVec, nInput, nOutput, batch_input, batch_output, learning_rate=0.01, num_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volleyball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
